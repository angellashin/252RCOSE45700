# main.py
from pathlib import Path
from dotenv import load_dotenv
import os
from typing import Dict, List, Any, Optional

from fastapi import FastAPI, HTTPException
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse, StreamingResponse
from pydantic import BaseModel

from langchain_community.document_loaders import PyPDFDirectoryLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma

from openai import OpenAI

# ========================
# 0. í™˜ê²½ ì„¤ì •
# ========================

load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

if not OPENAI_API_KEY:
    raise RuntimeError("OPENAI_API_KEYê°€ .envì— ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. .env íŒŒì¼ì— OPENAI_API_KEY=... ë¥¼ ì¶”ê°€í•˜ì„¸ìš”.")

client = OpenAI(api_key=OPENAI_API_KEY)

DATA_DIR = Path("data")
CHROMA_DIR = Path("chroma_db")

# LangChain ì„ë² ë”© + ë²¡í„°ìŠ¤í† ì–´ ì „ì—­ ë³€ìˆ˜
embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)
vectorstore: Optional[Chroma] = None
retriever = None

# ì„¸ì…˜ë³„ ëŒ€í™” íˆìŠ¤í† ë¦¬: session_id -> [ {role, content}, ... ]
CHAT_HISTORY: Dict[str, List[Dict[str, str]]] = {}

app = FastAPI(title="KU RAG Chatbot")


# ========================
# 1. LangChain RAG ì¸ë±ìŠ¤ ë¹Œë“œ
# ========================

def build_vectorstore_if_needed():
    """
    - data/ í´ë”ì˜ PDFë“¤ì„ LangChainìœ¼ë¡œ ë¡œë“œ
    - í…ìŠ¤íŠ¸ ì²­í¬ë¡œ ë‚˜ëˆˆ ë’¤
    - OpenAIEmbeddings + Chroma ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
    - ì´ë¯¸ chroma_dbê°€ ìˆìœ¼ë©´ ì¬ì‚¬ìš©
    """
    global vectorstore, retriever

    if retriever is not None and vectorstore is not None:
        # ì´ë¯¸ ì´ˆê¸°í™”ëœ ê²½ìš°
        return

    if not DATA_DIR.exists():
        raise RuntimeError("data í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê°•ì˜ê³„íšì„œ PDFë“¤ì„ data/ì— ë„£ì–´ì£¼ì„¸ìš”.")

    # ì´ë¯¸ ì €ì¥ëœ ë²¡í„°ìŠ¤í† ì–´ê°€ ìˆìœ¼ë©´ ê·¸ê±° ì¬ì‚¬ìš©
    if CHROMA_DIR.exists():
        print("ğŸ“‚ ê¸°ì¡´ Chroma ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì¤‘...")
        vectorstore = Chroma(
            persist_directory=str(CHROMA_DIR),
            embedding_function=embeddings,
        )
        retriever = vectorstore.as_retriever(search_kwargs={"k": 4})
        print("âœ… ê¸°ì¡´ ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì™„ë£Œ.")
        return

    # ì—†ìœ¼ë©´ ìƒˆë¡œ ë¹Œë“œ
    print("ğŸ“‚ PDF ë¡œë“œ ì‹œì‘ (LangChain PyPDFDirectoryLoader)...")
    loader = PyPDFDirectoryLoader(str(DATA_DIR))
    documents = loader.load()
    print(f"âœ… ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ. ë¬¸ì„œ ìˆ˜: {len(documents)}")

    print("âœ‚ï¸ í…ìŠ¤íŠ¸ ì²­í¬ ë¶„í•  (LangChain RecursiveCharacterTextSplitter)...")
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
    )
    splits = splitter.split_documents(documents)
    print(f"âœ… ì²­í¬ ë¶„í•  ì™„ë£Œ. ì²­í¬ ìˆ˜: {len(splits)}")

    print("ğŸ§  Chroma ë²¡í„°ìŠ¤í† ì–´ ìƒì„± (OpenAIEmbeddings ì‚¬ìš©)...")
    vectorstore = Chroma.from_documents(
        documents=splits,
        embedding=embeddings,
        persist_directory=str(CHROMA_DIR),
    )
    retriever = vectorstore.as_retriever(search_kwargs={"k": 4})
    print("âœ… ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ë° ì €ì¥ ì™„ë£Œ.")


def get_history_text(session_id: str, max_turns: int = 5) -> str:
    history = CHAT_HISTORY.get(session_id, [])
    history = history[-max_turns * 2 :]  # ìµœê·¼ max_turns ìŒë§Œ ìœ ì§€
    CHAT_HISTORY[session_id] = history

    lines: List[str] = []
    for msg in history:
        role = "ì‚¬ìš©ì" if msg["role"] == "user" else "AI"
        lines.append(f"{role}: {msg['content']}")
    return "\n".join(lines)


def add_to_history(session_id: str, role: str, content: str):
    if session_id not in CHAT_HISTORY:
        CHAT_HISTORY[session_id] = []
    CHAT_HISTORY[session_id].append({"role": role, "content": content})


# ========================
# 2. RAG ì§ˆì˜ (ë¹„-ìŠ¤íŠ¸ë¦¬ë°)
# ========================

def rag_answer(question: str, session_id: str) -> Dict[str, Any]:
    """
    LangChain:
      - Chroma retrieverë¡œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
      - ë©€í‹°í„´ íˆìŠ¤í† ë¦¬ í¬í•¨í•´ì„œ OpenAI LLM í˜¸ì¶œ
      - ë‹µë³€ + ì¶œì²˜ ëª©ë¡ ë°˜í™˜
    """
    build_vectorstore_if_needed()

    # ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ (LangChain retriever)
    docs = retriever.get_relevant_documents(question)

    context_parts: List[str] = []
    sources: List[Dict[str, Any]] = []

    for d in docs:
        src = d.metadata.get("source", "unknown")
        page = d.metadata.get("page", None)
        # PyPDFDirectoryLoaderì˜ sourceëŠ” ì „ì²´ ê²½ë¡œì¼ ìˆ˜ ìˆìŒ â†’ íŒŒì¼ëª…ë§Œ ì¶”ì¶œ
        filename = os.path.basename(src) if src else "unknown"
        context_parts.append(f"[ì¶œì²˜: {filename}, p.{page}]\n{d.page_content}\n")
        sources.append({"source": filename, "page": page})

    context_text = "\n\n".join(context_parts)
    history_text = get_history_text(session_id)

    system_prompt = (
        "ë‹¹ì‹ ì€ ëŒ€í•™ ê°•ì˜ ê³„íšì„œ ê´€ë ¨ ì§ˆë¬¸ì— ë‹µí•˜ëŠ” í•œêµ­ì–´ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. "
        "ë°˜ë“œì‹œ ì œê³µëœ ë¬¸ì„œ(context)ì™€ ëŒ€í™” íˆìŠ¤í† ë¦¬ë§Œì„ ê·¼ê±°ë¡œ ë‹µë³€í•˜ì„¸ìš”. "
        "ëª¨ë¥´ë©´ ëª¨ë¥¸ë‹¤ê³  ë§í•˜ì„¸ìš”."
    )

    user_content = f"""
ì´ì „ ëŒ€í™”:
{history_text if history_text else '(ì´ì „ ëŒ€í™” ì—†ìŒ)'}

ì‚¬ìš©ì ì§ˆë¬¸:
{question}

ê´€ë ¨ ë¬¸ì„œ(context):
{context_text}

ìœ„ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ ìì—°ìŠ¤ëŸ½ê³  ì¹œì ˆí•œ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”.
"""

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_content},
    ]

    resp = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=messages,
        temperature=0.1,
    )
    answer_text = resp.choices[0].message.content

    add_to_history(session_id, "user", question)
    add_to_history(session_id, "assistant", answer_text)

    # ì¶œì²˜ í…ìŠ¤íŠ¸ ë§Œë“¤ê¸°
    sources_text_lines: List[str] = []
    seen = set()
    for s in sources:
        key = (s["source"], s["page"])
        if key in seen:
            continue
        seen.add(key)
        sources_text_lines.append(f"- {s['source']} (p.{s['page']})")
    sources_text = "\n".join(sources_text_lines)

    return {
        "answer": answer_text,
        "sources_text": sources_text,
    }


# ========================
# 3. RAG ìŠ¤íŠ¸ë¦¬ë° ë²„ì „
# ========================

def rag_stream_answer(question: str, session_id: str):
    """
    - LangChain retrieverë¡œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
    - OpenAI ChatCompletion(stream=True)ë¡œ í† í° ë‹¨ìœ„ ìŠ¤íŠ¸ë¦¬ë°
    - ë§ˆì§€ë§‰ì— [ì°¸ê³  ë¬¸ì„œ] ì„¹ì…˜ í•¨ê»˜ ì „ì†¡
    """
    build_vectorstore_if_needed()

    docs = retriever.get_relevant_documents(question)

    context_parts: List[str] = []
    sources: List[Dict[str, Any]] = []
    for d in docs:
        src = d.metadata.get("source", "unknown")
        page = d.metadata.get("page", None)
        filename = os.path.basename(src) if src else "unknown"
        context_parts.append(f"[ì¶œì²˜: {filename}, p.{page}]\n{d.page_content}\n")
        sources.append({"source": filename, "page": page})

    context_text = "\n\n".join(context_parts)
    history_text = get_history_text(session_id)

    system_prompt = (
        "ë‹¹ì‹ ì€ ëŒ€í•™ ê°•ì˜ ê³„íšì„œ ê´€ë ¨ ì§ˆë¬¸ì— ë‹µí•˜ëŠ” í•œêµ­ì–´ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. "
        "ë°˜ë“œì‹œ ì œê³µëœ ë¬¸ì„œ(context)ì™€ ëŒ€í™” íˆìŠ¤í† ë¦¬ë§Œì„ ê·¼ê±°ë¡œ ë‹µë³€í•˜ì„¸ìš”. "
        "ëª¨ë¥´ë©´ ëª¨ë¥¸ë‹¤ê³  ë§í•˜ì„¸ìš”."
    )

    user_content = f"""
ì´ì „ ëŒ€í™”:
{history_text if history_text else '(ì´ì „ ëŒ€í™” ì—†ìŒ)'}

ì‚¬ìš©ì ì§ˆë¬¸:
{question}

ê´€ë ¨ ë¬¸ì„œ(context):
{context_text}

ìœ„ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ ìì—°ìŠ¤ëŸ½ê³  ì¹œì ˆí•œ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”.
"""

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_content},
    ]

    full_answer = ""

    stream = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=messages,
        temperature=0.1,
        stream=True,
    )

    # ë³¸ë¬¸ í† í° ìŠ¤íŠ¸ë¦¬ë°
    for chunk in stream:
        delta = chunk.choices[0].delta.content or ""
        full_answer += delta
        yield delta

    # íˆìŠ¤í† ë¦¬ì— ì €ì¥
    add_to_history(session_id, "user", question)
    add_to_history(session_id, "assistant", full_answer)

    # ì¶œì²˜ í…ìŠ¤íŠ¸
    sources_text_lines: List[str] = []
    seen = set()
    for s in sources:
        key = (s["source"], s["page"])
        if key in seen:
            continue
        seen.add(key)
        sources_text_lines.append(f"- {s['source']} (p.{s['page']})")
    sources_text = "\n".join(sources_text_lines)

    footer = "\n\n[ì°¸ê³  ë¬¸ì„œ]\n" + sources_text
    yield footer


# ========================
# 4. FastAPI: ì •ì  íŒŒì¼ (UI) + API ì—”ë“œí¬ì¸íŠ¸
# ========================

STATIC_DIR = Path("static")
STATIC_DIR.mkdir(exist_ok=True)
app.mount("/static", StaticFiles(directory=STATIC_DIR), name="static")


@app.get("/")
async def root():
    index_path = STATIC_DIR / "index.html"
    if index_path.exists():
        return FileResponse(index_path)
    return {"message": "RAG ì±—ë´‡ì´ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤. /static/index.htmlì„ ë§Œë“¤ì–´ UIë¥¼ ì¶”ê°€í•˜ì„¸ìš”."}


class ChatRequest(BaseModel):
    session_id: str
    question: str


class ChatResponse(BaseModel):
    answer: str
    sources_text: str


@app.post("/chat", response_model=ChatResponse)
async def chat(req: ChatRequest):
    try:
        result = rag_answer(req.question, req.session_id)
        return ChatResponse(
            answer=result["answer"],
            sources_text=result["sources_text"],
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/chat/stream")
async def chat_stream(req: ChatRequest):
    try:
        generator = rag_stream_answer(req.question, req.session_id)
        return StreamingResponse(generator, media_type="text/plain; charset=utf-8")
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
