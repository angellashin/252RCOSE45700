import os
from typing import List, Tuple

from dotenv import load_dotenv
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings

from openai import OpenAI

# ğŸ”¹ 0) í™˜ê²½ë³€ìˆ˜ ë¡œë“œ (ì—¬ê¸°ê¹Œì§„ ê°€ë²¼ì›€)
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

if not OPENAI_API_KEY:
    raise ValueError("OPENAI_API_KEYê°€ .envì— ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")

client = OpenAI(api_key=OPENAI_API_KEY)

# ì „ì—­ ëŒ€í™” íˆìŠ¤í† ë¦¬ (ì§ˆë¬¸, ë‹µë³€)
chat_history: List[Tuple[str, str]] = []


def init_rag():
    """
    RAGì—ì„œ ë¬´ê±°ìš´ ì´ˆê¸°í™” (ì„ë² ë”©, Chroma ë¡œë“œ)ë¥¼ ì—¬ê¸°ì„œë§Œ ìˆ˜í–‰.
    """
    print("1) í•œêµ­ì–´ ì„ë² ë”©(KoSimCSE) ë¡œë“œ ì¤‘...")
    embeddings = HuggingFaceEmbeddings(
        model_name="jhgan/ko-sroberta-multitask"
    )
    print("   âœ… ì„ë² ë”© ë¡œë“œ ì™„ë£Œ")

    print("2) Chroma ë²¡í„°ìŠ¤í† ì–´(index í´ë”) ë¡œë“œ ì¤‘...")
    db = Chroma(
        persist_directory="index",
        embedding_function=embeddings,
    )
    print("   âœ… Chroma ë¡œë“œ ì™„ë£Œ")

    retriever = db.as_retriever(search_kwargs={"k": 3})
    print("3) Retriever ì¤€ë¹„ ì™„ë£Œ")

    return retriever


def build_messages(question: str, context_text: str):
    messages = [
        {
            "role": "system",
            "content": (
                "ë„ˆëŠ” ëŒ€í•™ ê°•ì˜ê³„íšì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ëŠ” í•œêµ­ì–´ AI íŠœí„°ì•¼. "
                "ë°˜ë“œì‹œ ì œê³µëœ ë¬¸ì„œ ì •ë³´ë§Œ ì‚¬ìš©í•´ì„œ ë‹µë³€í•˜ê³ , "
                "ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ëª¨ë¥¸ë‹¤ê³  ë§í•´ì•¼ í•´."
            ),
        }
    ]

    # ê³¼ê±° ëŒ€í™” íˆìŠ¤í† ë¦¬ ë°˜ì˜
    for user_msg, bot_msg in chat_history:
        messages.append({"role": "user", "content": user_msg})
        messages.append({"role": "assistant", "content": bot_msg})

    # ì´ë²ˆ ì§ˆë¬¸ + ì»¨í…ìŠ¤íŠ¸
    messages.append(
        {
            "role": "user",
            "content": (
                "ë‹¤ìŒì€ ê°•ì˜ê³„íšì„œì—ì„œ ê²€ìƒ‰ëœ ê´€ë ¨ ë‚´ìš©ì´ì•¼:\n"
                f"{context_text}\n\n"
                f"ìœ„ ë‚´ìš©ì„ ì°¸ê³ í•´ì„œ, ë‹¤ìŒ ì§ˆë¬¸ì— í•œêµ­ì–´ë¡œ ìì„¸íˆ ë‹µë³€í•´ì¤˜:\n{question}"
            ),
        }
    )

    return messages


def ask(retriever, question: str) -> str:
    print("   ğŸ” ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ ì¤‘...")
    docs = retriever.get_relevant_documents(question)

    if not docs:
        answer = "ê´€ë ¨ëœ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆì–´ìš”. ì§ˆë¬¸ì„ ì¡°ê¸ˆ ë” êµ¬ì²´ì ìœ¼ë¡œ í•´ ì¤„ ìˆ˜ ìˆì„ê¹Œìš”?"
        chat_history.append((question, answer))
        return answer + "\n\nğŸ“š ì¶œì²˜:\n(ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ)"

    context_texts = []
    sources = []
    for d in docs:
        context_texts.append(d.page_content)
        src = d.metadata.get("source")
        if src and src not in sources:
            sources.append(src)

    context_str = "\n\n---\n\n".join(context_texts)

    print("   ğŸ¤– OpenAIì— ìš”ì²­ ë³´ë‚´ëŠ” ì¤‘...")
    messages = build_messages(question, context_str)

    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=messages,
        temperature=0.2,
    )

    answer = response.choices[0].message.content.strip()

    chat_history.append((question, answer))

    source_lines = "\n".join(f"- {s}" for s in sources)
    return f"{answer}\n\nğŸ“š ì¶œì²˜:\n{source_lines}"


if __name__ == "__main__":
    print("ğŸ’¬ RAG ì±—ë´‡ ì´ˆê¸°í™” ì‹œì‘...")

    # âœ… ë¬´ê±°ìš´ ì´ˆê¸°í™”ëŠ” ì—¬ê¸°ì—ì„œë§Œ!
    retriever = init_rag()

    print("\nâœ… RAG ì´ˆê¸°í™” ì™„ë£Œ!")
    print("ğŸ’¬ RAG ì±—ë´‡ ì‹œì‘! 'quit' ë˜ëŠ” 'exit' ì…ë ¥ ì‹œ ì¢…ë£Œ")

    while True:
        q = input("\nì§ˆë¬¸: ")
        if q.lower() in ["quit", "exit"]:
            print("ğŸ‘‹ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

        try:
            response = ask(retriever, q)
            print("\n" + response)
        except Exception as e:
            print(f"\nâš ï¸ ì—ëŸ¬ ë°œìƒ: {e}")
